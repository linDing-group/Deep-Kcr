{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from keras.layers import Input, Dense, Conv1D, Flatten, MaxPooling1D, Conv2D, MaxPooling2D, AveragePooling2D, Dropout, Reshape, normalization\n",
    "from keras.models import Model\n",
    "from keras.utils import to_categorical\n",
    "import keras.backend as K\n",
    "from keras.layers.recurrent import LSTM\n",
    "from sklearn import metrics\n",
    "import random\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    "    # Calculates the precision\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    # Calculates the recall\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1(test_Y, pre_test_y):\n",
    "    #calculate the F1-score\n",
    "    Precision = precision(test_Y, pre_test_y)\n",
    "    Recall = recall(test_Y, pre_test_y)\n",
    "    f1 = 2 * ((Precision * Recall) / (Precision + Recall + K.epsilon()))\n",
    "    return f1 \n",
    "\n",
    "def TP(test_Y,pre_test_y):\n",
    "    #calculate numbers of true positive samples\n",
    "    TP = K.sum(K.round(K.clip(test_Y * pre_test_y, 0, 1)))#TP\n",
    "    return TP\n",
    "\n",
    "def FN(test_Y,pre_test_y):\n",
    "     #calculate numbers of false negative samples\n",
    "    TP = K.sum(K.round(K.clip(test_Y * pre_test_y, 0, 1)))#TP\n",
    "    P=K.sum(K.round(K.clip(test_Y, 0, 1)))\n",
    "    FN = P-TP #FN=P-TP\n",
    "    return FN\n",
    "\n",
    "def TN(test_Y,pre_test_y):\n",
    "    #calculate numbers of True negative samples\n",
    "    TN=K.sum(K.round(K.clip((test_Y-K.ones_like(test_Y))*(pre_test_y-K.ones_like(pre_test_y)), 0, 1)))#TN\n",
    "    return TN\n",
    "\n",
    "def FP(test_Y,pre_test_y):\n",
    "    #calculate numbers of False positive samples\n",
    "    N = (-1)*K.sum(K.round(K.clip(test_Y-K.ones_like(test_Y), -1, 0)))#N\n",
    "    TN=K.sum(K.round(K.clip((test_Y-K.ones_like(test_Y))*(pre_test_y-K.ones_like(pre_test_y)), 0, 1)))#TN\n",
    "    FP=N-TN\n",
    "    return FP\n",
    "\n",
    "def dnn_model(train_X, train_Y, test_X, test_Y, lr, epoch, batch_size):\n",
    "    train_X = np.expand_dims(train_X, 2)\n",
    "    test_X = np.expand_dims(test_X, 2)\n",
    "    inputs = Input(shape = (train_X.shape[1], train_X.shape[2]))\n",
    "    x = Conv1D(32, kernel_size = 3, strides = 1, padding = 'valid', activation = 'relu')(inputs)\n",
    "    x = MaxPooling1D(pool_size = 2, strides = 2, padding = 'same')(x)\n",
    "    x = Flatten()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation = 'relu',kernel_regularizer = l2(1e-5))(x)\n",
    "    x = Dense(16, activation = 'relu',kernel_regularizer = l2(1e-5))(x)\n",
    "    x = Dense(8, activation = 'relu',kernel_regularizer = l2(1e-5))(x)\n",
    "    predictions = Dense(1, activation = 'sigmoid')(x)\n",
    "    model = Model(inputs = inputs, outputs = predictions)\n",
    "    print(\"model\")\n",
    "    model.compile(optimizer = 'RMSProp',\n",
    "                  loss = 'mean_squared_error',\n",
    "                  metrics = ['acc',precision,recall,f1,TP,FN,TN,FP])\n",
    "    print(\"compile\")\n",
    "    model.fit(train_X, train_Y, epochs = epoch, batch_size = 32, validation_data = (test_X, test_Y), shuffle = True)\n",
    "    model.save('6fea_idx12Scale_infoGainSorted50.h5') #save model\n",
    "    pre_test_y = model.predict(test_X, batch_size = 50)\n",
    "    pre_train_y = model.predict(train_X, batch_size = 50)\n",
    "    test_auc = metrics.roc_auc_score(test_Y, pre_test_y)\n",
    "    train_auc = metrics.roc_auc_score(train_Y, pre_train_y)\n",
    "    print(\"train_auc: \", train_auc)\n",
    "    print(\"test_auc: \", test_auc) \n",
    "    return test_auc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.315789  0.428571  0.623001  ... 0.620922  0.166667  0.       ]\n",
      " [0.315789  0.428571  0.627306  ... 0.580021  0.166667  0.       ]\n",
      " [0.421053  0.428571  0.311193  ... 0.486997  0.166667  0.       ]\n",
      " ...\n",
      " [0.578947  0.190476  0.777983  ... 0.729916  0.        0.166667 ]\n",
      " [0.368421  0.238095  0.623001  ... 0.482584  0.25      0.0833333]\n",
      " [0.315789  0.047619  0.623001  ... 0.132024  0.0833333 0.0833333]]\n",
      "X.shape:  (13950, 48)\n",
      "Y.shape:  (13950,)\n",
      "\n",
      "\n",
      "i:  0\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 1s 119us/step - loss: 0.2074 - acc: 0.6761 - precision: 0.6468 - recall: 0.7643 - f1: 0.6829 - TP: 12.2595 - FN: 3.7583 - TN: 9.3410 - FP: 6.5878 - val_loss: 0.1905 - val_acc: 0.7118 - val_precision: 0.4823 - val_recall: 0.4027 - val_f1: 0.4383 - val_TP: 12.7500 - val_FN: 2.7045 - val_TN: 9.8182 - val_FP: 6.4318\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 100us/step - loss: 0.1895 - acc: 0.7141 - precision: 0.6853 - recall: 0.8070 - f1: 0.7327 - TP: 12.9262 - FN: 3.0916 - TN: 9.8880 - FP: 6.0407 - val_loss: 0.1910 - val_acc: 0.7025 - val_precision: 0.4886 - val_recall: 0.4531 - val_f1: 0.4675 - val_TP: 13.9545 - val_FN: 1.5000 - val_TN: 8.3182 - val_FP: 7.9318\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 113us/step - loss: 0.1854 - acc: 0.7228 - precision: 0.6945 - recall: 0.8122 - f1: 0.7386 - TP: 13.0204 - FN: 2.9975 - TN: 10.0712 - FP: 5.8575 - val_loss: 0.1848 - val_acc: 0.7168 - val_precision: 0.4838 - val_recall: 0.3395 - val_f1: 0.3974 - val_TP: 10.7273 - val_FN: 4.7273 - val_TN: 12.0000 - val_FP: 4.2500\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 101us/step - loss: 0.1811 - acc: 0.7335 - precision: 0.7051 - recall: 0.8108 - f1: 0.7463 - TP: 12.9720 - FN: 3.0458 - TN: 10.4606 - FP: 5.4682 - val_loss: 0.1832 - val_acc: 0.7211 - val_precision: 0.4811 - val_recall: 0.3352 - val_f1: 0.3937 - val_TP: 10.6591 - val_FN: 4.7955 - val_TN: 12.2045 - val_FP: 4.0455\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 97us/step - loss: 0.1789 - acc: 0.7379 - precision: 0.7125 - recall: 0.8039 - f1: 0.7480 - TP: 12.8677 - FN: 3.1501 - TN: 10.7048 - FP: 5.2239 - val_loss: 0.1775 - val_acc: 0.7376 - val_precision: 0.4848 - val_recall: 0.3928 - val_f1: 0.4331 - val_TP: 12.3636 - val_FN: 3.0909 - val_TN: 11.0227 - val_FP: 5.2273\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 100us/step - loss: 0.1771 - acc: 0.7392 - precision: 0.7143 - recall: 0.8073 - f1: 0.7500 - TP: 12.9288 - FN: 3.0891 - TN: 10.6870 - FP: 5.2417 - val_loss: 0.1756 - val_acc: 0.7419 - val_precision: 0.4876 - val_recall: 0.3963 - val_f1: 0.4356 - val_TP: 12.3409 - val_FN: 3.1136 - val_TN: 11.1818 - val_FP: 5.0682\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 107us/step - loss: 0.1772 - acc: 0.7400 - precision: 0.7136 - recall: 0.8059 - f1: 0.7509 - TP: 12.9288 - FN: 3.0891 - TN: 10.7125 - FP: 5.2163 - val_loss: 0.1779 - val_acc: 0.7333 - val_precision: 0.4867 - val_recall: 0.4219 - val_f1: 0.4508 - val_TP: 13.1591 - val_FN: 2.2955 - val_TN: 10.0909 - val_FP: 6.1591\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 98us/step - loss: 0.1767 - acc: 0.7410 - precision: 0.7157 - recall: 0.8121 - f1: 0.7524 - TP: 12.9975 - FN: 3.0204 - TN: 10.6743 - FP: 5.2545 - val_loss: 0.1752 - val_acc: 0.7434 - val_precision: 0.4876 - val_recall: 0.3991 - val_f1: 0.4376 - val_TP: 12.4318 - val_FN: 3.0227 - val_TN: 11.1364 - val_FP: 5.1136\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 95us/step - loss: 0.1755 - acc: 0.7457 - precision: 0.7208 - recall: 0.8061 - f1: 0.7544 - TP: 12.9160 - FN: 3.1018 - TN: 10.9059 - FP: 5.0229 - val_loss: 0.1738 - val_acc: 0.7391 - val_precision: 0.4864 - val_recall: 0.3935 - val_f1: 0.4337 - val_TP: 12.3182 - val_FN: 3.1364 - val_TN: 11.1136 - val_FP: 5.1364\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 96us/step - loss: 0.1749 - acc: 0.7431 - precision: 0.7190 - recall: 0.8017 - f1: 0.7511 - TP: 12.8499 - FN: 3.1679 - TN: 10.8906 - FP: 5.0382 - val_loss: 0.1766 - val_acc: 0.7333 - val_precision: 0.4838 - val_recall: 0.3459 - val_f1: 0.4025 - val_TP: 10.9318 - val_FN: 4.5227 - val_TN: 12.3182 - val_FP: 3.9318\n",
      "train_auc:  0.8343586750476443\n",
      "test_auc:  0.8169600987248047\n",
      "\n",
      "\n",
      "i:  1\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 2s 128us/step - loss: 0.2081 - acc: 0.6743 - precision: 0.6499 - recall: 0.7356 - f1: 0.6683 - TP: 11.6718 - FN: 4.2137 - TN: 9.8702 - FP: 6.1908 - val_loss: 0.1985 - val_acc: 0.6939 - val_precision: 0.5212 - val_recall: 0.4943 - val_f1: 0.5070 - val_TP: 15.7273 - val_FN: 0.9091 - val_TN: 6.2727 - val_FP: 8.7955\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1924 - acc: 0.7086 - precision: 0.6790 - recall: 0.7935 - f1: 0.7221 - TP: 12.5980 - FN: 3.2875 - TN: 10.0407 - FP: 6.0204 - val_loss: 0.1800 - val_acc: 0.7369 - val_precision: 0.5212 - val_recall: 0.4786 - val_f1: 0.4985 - val_TP: 15.2273 - val_FN: 1.4091 - val_TN: 8.1364 - val_FP: 6.9318\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 105us/step - loss: 0.1877 - acc: 0.7195 - precision: 0.6886 - recall: 0.8041 - f1: 0.7334 - TP: 12.7812 - FN: 3.1043 - TN: 10.2036 - FP: 5.8575 - val_loss: 0.1799 - val_acc: 0.7484 - val_precision: 0.5217 - val_recall: 0.3707 - val_f1: 0.4318 - val_TP: 11.7955 - val_FN: 4.8409 - val_TN: 11.9318 - val_FP: 3.1364\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1859 - acc: 0.7241 - precision: 0.6949 - recall: 0.8025 - f1: 0.7350 - TP: 12.7583 - FN: 3.1272 - TN: 10.3740 - FP: 5.6870 - val_loss: 0.1737 - val_acc: 0.7584 - val_precision: 0.5218 - val_recall: 0.4015 - val_f1: 0.4528 - val_TP: 12.7727 - val_FN: 3.8636 - val_TN: 11.2727 - val_FP: 3.7955\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1838 - acc: 0.7279 - precision: 0.6973 - recall: 0.8116 - f1: 0.7418 - TP: 12.8982 - FN: 2.9873 - TN: 10.3562 - FP: 5.7048 - val_loss: 0.1827 - val_acc: 0.7290 - val_precision: 0.5216 - val_recall: 0.3357 - val_f1: 0.4073 - val_TP: 10.6818 - val_FN: 5.9545 - val_TN: 12.4318 - val_FP: 2.6364\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 105us/step - loss: 0.1809 - acc: 0.7296 - precision: 0.7026 - recall: 0.8056 - f1: 0.7420 - TP: 12.7684 - FN: 3.1170 - TN: 10.5394 - FP: 5.5216 - val_loss: 0.1918 - val_acc: 0.7061 - val_precision: 0.5227 - val_recall: 0.2978 - val_f1: 0.3765 - val_TP: 9.4773 - val_FN: 7.1591 - val_TN: 12.9091 - val_FP: 2.1591\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 105us/step - loss: 0.1808 - acc: 0.7337 - precision: 0.7077 - recall: 0.8006 - f1: 0.7432 - TP: 12.7252 - FN: 3.1603 - TN: 10.7125 - FP: 5.3486 - val_loss: 0.1777 - val_acc: 0.7391 - val_precision: 0.5215 - val_recall: 0.3477 - val_f1: 0.4159 - val_TP: 11.0682 - val_FN: 5.5682 - val_TN: 12.3636 - val_FP: 2.7045\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 105us/step - loss: 0.1803 - acc: 0.7310 - precision: 0.7035 - recall: 0.7966 - f1: 0.7391 - TP: 12.6794 - FN: 3.2061 - TN: 10.6743 - FP: 5.3868 - val_loss: 0.1647 - val_acc: 0.7656 - val_precision: 0.5218 - val_recall: 0.4391 - val_f1: 0.4762 - val_TP: 13.9773 - val_FN: 2.6591 - val_TN: 10.2955 - val_FP: 4.7727\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 105us/step - loss: 0.1799 - acc: 0.7337 - precision: 0.7054 - recall: 0.8039 - f1: 0.7428 - TP: 12.7608 - FN: 3.1247 - TN: 10.6768 - FP: 5.3842 - val_loss: 0.1665 - val_acc: 0.7670 - val_precision: 0.5218 - val_recall: 0.4057 - val_f1: 0.4557 - val_TP: 12.9091 - val_FN: 3.7273 - val_TN: 11.4091 - val_FP: 3.6591\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 110us/step - loss: 0.1794 - acc: 0.7355 - precision: 0.7079 - recall: 0.8006 - f1: 0.7438 - TP: 12.7277 - FN: 3.1578 - TN: 10.7684 - FP: 5.2926 - val_loss: 0.1647 - val_acc: 0.7591 - val_precision: 0.5217 - val_recall: 0.4206 - val_f1: 0.4652 - val_TP: 13.3864 - val_FN: 3.2500 - val_TN: 10.6818 - val_FP: 4.3864\n",
      "train_auc:  0.8246738882402537\n",
      "test_auc:  0.8395169332970683\n",
      "\n",
      "\n",
      "i:  2\n",
      "model\n",
      "compile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 2s 123us/step - loss: 0.2133 - acc: 0.6589 - precision: 0.6430 - recall: 0.7668 - f1: 0.6805 - TP: 12.2366 - FN: 3.7455 - TN: 8.8142 - FP: 7.1501 - val_loss: 0.1857 - val_acc: 0.7276 - val_precision: 0.4959 - val_recall: 0.4121 - val_f1: 0.4496 - val_TP: 13.0000 - val_FN: 2.7727 - val_TN: 10.0682 - val_FP: 5.8636\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 99us/step - loss: 0.1920 - acc: 0.7121 - precision: 0.6856 - recall: 0.7967 - f1: 0.7271 - TP: 12.7226 - FN: 3.2595 - TN: 10.0280 - FP: 5.9364 - val_loss: 0.1807 - val_acc: 0.7355 - val_precision: 0.4959 - val_recall: 0.4092 - val_f1: 0.4477 - val_TP: 12.9091 - val_FN: 2.8636 - val_TN: 10.4091 - val_FP: 5.5227\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 98us/step - loss: 0.1851 - acc: 0.7225 - precision: 0.6991 - recall: 0.7927 - f1: 0.7346 - TP: 12.6616 - FN: 3.3206 - TN: 10.4198 - FP: 5.5445 - val_loss: 0.1792 - val_acc: 0.7341 - val_precision: 0.4953 - val_recall: 0.4443 - val_f1: 0.4680 - val_TP: 14.0227 - val_FN: 1.7500 - val_TN: 9.2500 - val_FP: 6.6818\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 98us/step - loss: 0.1820 - acc: 0.7309 - precision: 0.7057 - recall: 0.7960 - f1: 0.7408 - TP: 12.7405 - FN: 3.2417 - TN: 10.6081 - FP: 5.3562 - val_loss: 0.1750 - val_acc: 0.7462 - val_precision: 0.4951 - val_recall: 0.4348 - val_f1: 0.4625 - val_TP: 13.7273 - val_FN: 2.0455 - val_TN: 9.9318 - val_FP: 6.0000\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 101us/step - loss: 0.1789 - acc: 0.7347 - precision: 0.7087 - recall: 0.7958 - f1: 0.7417 - TP: 12.7532 - FN: 3.2290 - TN: 10.7176 - FP: 5.2468 - val_loss: 0.1734 - val_acc: 0.7477 - val_precision: 0.4968 - val_recall: 0.3815 - val_f1: 0.4304 - val_TP: 12.0227 - val_FN: 3.7500 - val_TN: 11.6818 - val_FP: 4.2500\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 2s 120us/step - loss: 0.1776 - acc: 0.7385 - precision: 0.7180 - recall: 0.8004 - f1: 0.7489 - TP: 12.7939 - FN: 3.1883 - TN: 10.7990 - FP: 5.1654 - val_loss: 0.1705 - val_acc: 0.7477 - val_precision: 0.4968 - val_recall: 0.3815 - val_f1: 0.4307 - val_TP: 12.0227 - val_FN: 3.7500 - val_TN: 11.6818 - val_FP: 4.2500\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 115us/step - loss: 0.1773 - acc: 0.7355 - precision: 0.7116 - recall: 0.8019 - f1: 0.7464 - TP: 12.7990 - FN: 3.1832 - TN: 10.6972 - FP: 5.2672 - val_loss: 0.1711 - val_acc: 0.7470 - val_precision: 0.4968 - val_recall: 0.4163 - val_f1: 0.4524 - val_TP: 13.1364 - val_FN: 2.6364 - val_TN: 10.5455 - val_FP: 5.3864\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 110us/step - loss: 0.1755 - acc: 0.7430 - precision: 0.7203 - recall: 0.7992 - f1: 0.7499 - TP: 12.7735 - FN: 3.2087 - TN: 10.9618 - FP: 5.0025 - val_loss: 0.1697 - val_acc: 0.7584 - val_precision: 0.4953 - val_recall: 0.4401 - val_f1: 0.4655 - val_TP: 13.8864 - val_FN: 1.8864 - val_TN: 10.1591 - val_FP: 5.7727\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 111us/step - loss: 0.1747 - acc: 0.7450 - precision: 0.7253 - recall: 0.8004 - f1: 0.7531 - TP: 12.7761 - FN: 3.2061 - TN: 11.0229 - FP: 4.9415 - val_loss: 0.1662 - val_acc: 0.7599 - val_precision: 0.4959 - val_recall: 0.4121 - val_f1: 0.4493 - val_TP: 13.0000 - val_FN: 2.7727 - val_TN: 11.0909 - val_FP: 4.8409\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 106us/step - loss: 0.1734 - acc: 0.7482 - precision: 0.7258 - recall: 0.8026 - f1: 0.7558 - TP: 12.8092 - FN: 3.1730 - TN: 11.0941 - FP: 4.8702 - val_loss: 0.1671 - val_acc: 0.7620 - val_precision: 0.4969 - val_recall: 0.4344 - val_f1: 0.4629 - val_TP: 13.7045 - val_FN: 2.0682 - val_TN: 10.4545 - val_FP: 5.4773\n",
      "train_auc:  0.8367566681183548\n",
      "test_auc:  0.8307605026989028\n",
      "\n",
      "\n",
      "i:  3\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 2s 133us/step - loss: 0.2052 - acc: 0.6785 - precision: 0.6521 - recall: 0.7600 - f1: 0.6860 - TP: 12.1858 - FN: 3.8219 - TN: 9.4911 - FP: 6.4478 - val_loss: 0.1935 - val_acc: 0.7032 - val_precision: 0.4869 - val_recall: 0.4562 - val_f1: 0.4683 - val_TP: 14.1818 - val_FN: 1.3636 - val_TN: 8.1136 - val_FP: 8.0455\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 111us/step - loss: 0.1882 - acc: 0.7188 - precision: 0.6919 - recall: 0.7977 - f1: 0.7322 - TP: 12.7939 - FN: 3.2137 - TN: 10.1679 - FP: 5.7710 - val_loss: 0.1830 - val_acc: 0.7276 - val_precision: 0.4886 - val_recall: 0.3525 - val_f1: 0.4075 - val_TP: 10.9773 - val_FN: 4.5682 - val_TN: 12.0909 - val_FP: 4.0682\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1845 - acc: 0.7287 - precision: 0.7045 - recall: 0.8029 - f1: 0.7415 - TP: 12.8117 - FN: 3.1959 - TN: 10.4682 - FP: 5.4707 - val_loss: 0.1961 - val_acc: 0.6982 - val_precision: 0.4909 - val_recall: 0.2670 - val_f1: 0.3436 - val_TP: 8.3182 - val_FN: 7.2273 - val_TN: 13.8182 - val_FP: 2.3409\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1815 - acc: 0.7321 - precision: 0.7061 - recall: 0.8015 - f1: 0.7428 - TP: 12.8397 - FN: 3.1679 - TN: 10.5471 - FP: 5.3919 - val_loss: 0.1930 - val_acc: 0.7125 - val_precision: 0.4878 - val_recall: 0.4652 - val_f1: 0.4734 - val_TP: 14.4318 - val_FN: 1.1136 - val_TN: 8.1591 - val_FP: 8.0000\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1800 - acc: 0.7344 - precision: 0.7121 - recall: 0.7977 - f1: 0.7447 - TP: 12.7710 - FN: 3.2366 - TN: 10.6921 - FP: 5.2468 - val_loss: 0.1766 - val_acc: 0.7398 - val_precision: 0.4886 - val_recall: 0.3750 - val_f1: 0.4223 - val_TP: 11.6591 - val_FN: 3.8864 - val_TN: 11.7955 - val_FP: 4.3636\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1777 - acc: 0.7378 - precision: 0.7168 - recall: 0.7948 - f1: 0.7453 - TP: 12.7099 - FN: 3.2977 - TN: 10.8601 - FP: 5.0789 - val_loss: 0.1748 - val_acc: 0.7384 - val_precision: 0.4886 - val_recall: 0.4216 - val_f1: 0.4507 - val_TP: 13.1136 - val_FN: 2.4318 - val_TN: 10.2955 - val_FP: 5.8636\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1777 - acc: 0.7403 - precision: 0.7170 - recall: 0.7984 - f1: 0.7483 - TP: 12.8015 - FN: 3.2061 - TN: 10.8499 - FP: 5.0891 - val_loss: 0.1747 - val_acc: 0.7455 - val_precision: 0.4874 - val_recall: 0.3759 - val_f1: 0.4226 - val_TP: 11.7273 - val_FN: 3.8182 - val_TN: 11.9091 - val_FP: 4.2500\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1778 - acc: 0.7379 - precision: 0.7146 - recall: 0.7967 - f1: 0.7458 - TP: 12.7481 - FN: 3.2595 - TN: 10.8244 - FP: 5.1145 - val_loss: 0.1720 - val_acc: 0.7455 - val_precision: 0.4906 - val_recall: 0.3783 - val_f1: 0.4251 - val_TP: 11.7273 - val_FN: 3.8182 - val_TN: 11.9091 - val_FP: 4.2500\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1763 - acc: 0.7418 - precision: 0.7200 - recall: 0.7985 - f1: 0.7497 - TP: 12.7863 - FN: 3.2214 - TN: 10.9109 - FP: 5.0280 - val_loss: 0.1709 - val_acc: 0.7484 - val_precision: 0.4899 - val_recall: 0.3875 - val_f1: 0.4305 - val_TP: 12.0227 - val_FN: 3.5227 - val_TN: 11.7045 - val_FP: 4.4545\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1747 - acc: 0.7452 - precision: 0.7210 - recall: 0.8030 - f1: 0.7520 - TP: 12.8804 - FN: 3.1272 - TN: 10.9262 - FP: 5.0127 - val_loss: 0.1712 - val_acc: 0.7477 - val_precision: 0.4892 - val_recall: 0.3939 - val_f1: 0.4340 - val_TP: 12.2273 - val_FN: 3.3182 - val_TN: 11.4773 - val_FP: 4.6818\n",
      "train_auc:  0.8327656524666895\n",
      "test_auc:  0.8246477656870728\n",
      "\n",
      "\n",
      "i:  4\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 2s 130us/step - loss: 0.2142 - acc: 0.6558 - precision: 0.6382 - recall: 0.7483 - f1: 0.6676 - TP: 11.8372 - FN: 4.0509 - TN: 9.1145 - FP: 6.9440 - val_loss: 0.1926 - val_acc: 0.7154 - val_precision: 0.5216 - val_recall: 0.3819 - val_f1: 0.4398 - val_TP: 12.1364 - val_FN: 4.4773 - val_TN: 10.5455 - val_FP: 4.5455\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 101us/step - loss: 0.1933 - acc: 0.7052 - precision: 0.6766 - recall: 0.7896 - f1: 0.7164 - TP: 12.5420 - FN: 3.3461 - TN: 9.9873 - FP: 6.0712 - val_loss: 0.1836 - val_acc: 0.7254 - val_precision: 0.5215 - val_recall: 0.3795 - val_f1: 0.4380 - val_TP: 12.0682 - val_FN: 4.5455 - val_TN: 10.9318 - val_FP: 4.1591\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1883 - acc: 0.7171 - precision: 0.6876 - recall: 0.7933 - f1: 0.7267 - TP: 12.6387 - FN: 3.2494 - TN: 10.2697 - FP: 5.7888 - val_loss: 0.1765 - val_acc: 0.7498 - val_precision: 0.5216 - val_recall: 0.4302 - val_f1: 0.4709 - val_TP: 13.6818 - val_FN: 2.9318 - val_TN: 10.0909 - val_FP: 5.0000\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1842 - acc: 0.7237 - precision: 0.6961 - recall: 0.7985 - f1: 0.7349 - TP: 12.6641 - FN: 3.2239 - TN: 10.4555 - FP: 5.6031 - val_loss: 0.1734 - val_acc: 0.7570 - val_precision: 0.5217 - val_recall: 0.4510 - val_f1: 0.4832 - val_TP: 14.3409 - val_FN: 2.2727 - val_TN: 9.6591 - val_FP: 5.4318\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1816 - acc: 0.7315 - precision: 0.7067 - recall: 0.8013 - f1: 0.7420 - TP: 12.6997 - FN: 3.1883 - TN: 10.6692 - FP: 5.3893 - val_loss: 0.1714 - val_acc: 0.7513 - val_precision: 0.5217 - val_recall: 0.4518 - val_f1: 0.4837 - val_TP: 14.3636 - val_FN: 2.2500 - val_TN: 9.4545 - val_FP: 5.6364\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1794 - acc: 0.7337 - precision: 0.7081 - recall: 0.7998 - f1: 0.7425 - TP: 12.6921 - FN: 3.1959 - TN: 10.7481 - FP: 5.3104 - val_loss: 0.1706 - val_acc: 0.7599 - val_precision: 0.5217 - val_recall: 0.4381 - val_f1: 0.4757 - val_TP: 13.9318 - val_FN: 2.6818 - val_TN: 10.1591 - val_FP: 4.9318\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 103us/step - loss: 0.1769 - acc: 0.7413 - precision: 0.7161 - recall: 0.8038 - f1: 0.7488 - TP: 12.7455 - FN: 3.1425 - TN: 10.9364 - FP: 5.1221 - val_loss: 0.1697 - val_acc: 0.7627 - val_precision: 0.5217 - val_recall: 0.4390 - val_f1: 0.4761 - val_TP: 13.9545 - val_FN: 2.6591 - val_TN: 10.2273 - val_FP: 4.8636\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1784 - acc: 0.7364 - precision: 0.7131 - recall: 0.7985 - f1: 0.7442 - TP: 12.6514 - FN: 3.2366 - TN: 10.8728 - FP: 5.1858 - val_loss: 0.1699 - val_acc: 0.7613 - val_precision: 0.5216 - val_recall: 0.4173 - val_f1: 0.4628 - val_TP: 13.2727 - val_FN: 3.3409 - val_TN: 10.8636 - val_FP: 4.2273\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1744 - acc: 0.7461 - precision: 0.7200 - recall: 0.8019 - f1: 0.7517 - TP: 12.7455 - FN: 3.1425 - TN: 11.0891 - FP: 4.9695 - val_loss: 0.1692 - val_acc: 0.7591 - val_precision: 0.5217 - val_recall: 0.4383 - val_f1: 0.4756 - val_TP: 13.9318 - val_FN: 2.6818 - val_TN: 10.1364 - val_FP: 4.9545\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1752 - acc: 0.7432 - precision: 0.7180 - recall: 0.8047 - f1: 0.7521 - TP: 12.7659 - FN: 3.1221 - TN: 10.9771 - FP: 5.0814 - val_loss: 0.1740 - val_acc: 0.7462 - val_precision: 0.5218 - val_recall: 0.4633 - val_f1: 0.4903 - val_TP: 14.7273 - val_FN: 1.8864 - val_TN: 8.9318 - val_FP: 6.1591\n",
      "train_auc:  0.8358740791096071\n",
      "test_auc:  0.8289189590097736\n",
      "\n",
      "\n",
      "i:  5\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 2s 129us/step - loss: 0.2369 - acc: 0.6320 - precision: 0.5966 - recall: 0.8963 - f1: 0.7040 - TP: 14.3384 - FN: 1.6641 - TN: 5.8524 - FP: 10.0916 - val_loss: 0.2098 - val_acc: 0.7032 - val_precision: 0.4912 - val_recall: 0.4518 - val_f1: 0.4699 - val_TP: 14.1364 - val_FN: 1.4545 - val_TN: 8.1591 - val_FP: 7.9545\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 103us/step - loss: 0.1999 - acc: 0.6995 - precision: 0.6781 - recall: 0.7722 - f1: 0.7103 - TP: 12.3868 - FN: 3.6158 - TN: 9.9593 - FP: 5.9847 - val_loss: 0.1813 - val_acc: 0.7319 - val_precision: 0.4912 - val_recall: 0.4312 - val_f1: 0.4579 - val_TP: 13.4773 - val_FN: 2.1136 - val_TN: 9.7273 - val_FP: 6.3864\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 103us/step - loss: 0.1912 - acc: 0.7120 - precision: 0.6876 - recall: 0.7880 - f1: 0.7245 - TP: 12.6107 - FN: 3.3919 - TN: 10.1349 - FP: 5.8092 - val_loss: 0.1766 - val_acc: 0.7398 - val_precision: 0.4924 - val_recall: 0.3962 - val_f1: 0.4379 - val_TP: 12.3864 - val_FN: 3.2045 - val_TN: 11.0682 - val_FP: 5.0455\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 103us/step - loss: 0.1872 - acc: 0.7205 - precision: 0.6933 - recall: 0.7993 - f1: 0.7330 - TP: 12.7939 - FN: 3.2087 - TN: 10.2239 - FP: 5.7201 - val_loss: 0.1796 - val_acc: 0.7319 - val_precision: 0.4916 - val_recall: 0.4499 - val_f1: 0.4688 - val_TP: 14.0455 - val_FN: 1.5455 - val_TN: 9.1591 - val_FP: 6.9545\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1847 - acc: 0.7250 - precision: 0.6993 - recall: 0.7996 - f1: 0.7384 - TP: 12.7888 - FN: 3.2137 - TN: 10.3740 - FP: 5.5700 - val_loss: 0.1829 - val_acc: 0.7168 - val_precision: 0.4909 - val_recall: 0.3102 - val_f1: 0.3784 - val_TP: 9.7500 - val_FN: 5.8409 - val_TN: 12.9773 - val_FP: 3.1364\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 103us/step - loss: 0.1828 - acc: 0.7291 - precision: 0.7060 - recall: 0.7982 - f1: 0.7413 - TP: 12.7812 - FN: 3.2214 - TN: 10.5115 - FP: 5.4326 - val_loss: 0.1730 - val_acc: 0.7477 - val_precision: 0.4906 - val_recall: 0.4296 - val_f1: 0.4571 - val_TP: 13.4545 - val_FN: 2.1364 - val_TN: 10.2500 - val_FP: 5.8636\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 103us/step - loss: 0.1814 - acc: 0.7341 - precision: 0.7092 - recall: 0.8034 - f1: 0.7458 - TP: 12.8372 - FN: 3.1654 - TN: 10.6158 - FP: 5.3282 - val_loss: 0.1768 - val_acc: 0.7369 - val_precision: 0.4906 - val_recall: 0.4452 - val_f1: 0.4662 - val_TP: 13.9545 - val_FN: 1.6364 - val_TN: 9.4091 - val_FP: 6.7045\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1799 - acc: 0.7361 - precision: 0.7091 - recall: 0.8045 - f1: 0.7462 - TP: 12.8880 - FN: 3.1145 - TN: 10.6285 - FP: 5.3155 - val_loss: 0.1717 - val_acc: 0.7599 - val_precision: 0.4935 - val_recall: 0.3891 - val_f1: 0.4344 - val_TP: 12.1591 - val_FN: 3.4318 - val_TN: 11.9318 - val_FP: 4.1818\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 105us/step - loss: 0.1780 - acc: 0.7409 - precision: 0.7133 - recall: 0.8097 - f1: 0.7517 - TP: 12.9822 - FN: 3.0204 - TN: 10.6870 - FP: 5.2570 - val_loss: 0.1697 - val_acc: 0.7484 - val_precision: 0.4915 - val_recall: 0.4189 - val_f1: 0.4513 - val_TP: 13.1136 - val_FN: 2.4773 - val_TN: 10.6136 - val_FP: 5.5000\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1767 - acc: 0.7431 - precision: 0.7170 - recall: 0.8008 - f1: 0.7503 - TP: 12.8422 - FN: 3.1603 - TN: 10.8957 - FP: 5.0483 - val_loss: 0.1690 - val_acc: 0.7527 - val_precision: 0.4948 - val_recall: 0.3905 - val_f1: 0.4354 - val_TP: 12.2045 - val_FN: 3.3864 - val_TN: 11.6591 - val_FP: 4.4545\n",
      "train_auc:  0.8262941638050255\n",
      "test_auc:  0.8294306439077748\n",
      "\n",
      "\n",
      "i:  6\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 2s 126us/step - loss: 0.2179 - acc: 0.6468 - precision: 0.6351 - recall: 0.6794 - f1: 0.6229 - TP: 10.6947 - FN: 5.1908 - TN: 9.9695 - FP: 6.0916 - val_loss: 0.2706 - val_acc: 0.5183 - val_precision: 0.4773 - val_recall: 0.0528 - val_f1: 0.0933 - val_TP: 1.6818 - val_FN: 14.9545 - val_TN: 14.7500 - val_FP: 0.3182\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1964 - acc: 0.7000 - precision: 0.6751 - recall: 0.7815 - f1: 0.7110 - TP: 12.4224 - FN: 3.4631 - TN: 9.9415 - FP: 6.1196 - val_loss: 0.1911 - val_acc: 0.7118 - val_precision: 0.5227 - val_recall: 0.3584 - val_f1: 0.4236 - val_TP: 11.4091 - val_FN: 5.2273 - val_TN: 11.1591 - val_FP: 3.9091\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12555/12555 [==============================] - 1s 99us/step - loss: 0.1880 - acc: 0.7189 - precision: 0.6924 - recall: 0.7952 - f1: 0.7299 - TP: 12.6387 - FN: 3.2468 - TN: 10.3282 - FP: 5.7328 - val_loss: 0.1814 - val_acc: 0.7341 - val_precision: 0.5227 - val_recall: 0.4155 - val_f1: 0.4622 - val_TP: 13.2273 - val_FN: 3.4091 - val_TN: 10.0455 - val_FP: 5.0227\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 99us/step - loss: 0.1834 - acc: 0.7286 - precision: 0.7012 - recall: 0.7958 - f1: 0.7370 - TP: 12.6565 - FN: 3.2290 - TN: 10.6209 - FP: 5.4402 - val_loss: 0.1798 - val_acc: 0.7376 - val_precision: 0.5227 - val_recall: 0.4013 - val_f1: 0.4533 - val_TP: 12.7727 - val_FN: 3.8636 - val_TN: 10.6136 - val_FP: 4.4545\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 99us/step - loss: 0.1799 - acc: 0.7352 - precision: 0.7096 - recall: 0.8003 - f1: 0.7441 - TP: 12.6947 - FN: 3.1908 - TN: 10.7913 - FP: 5.2697 - val_loss: 0.1753 - val_acc: 0.7462 - val_precision: 0.5227 - val_recall: 0.4341 - val_f1: 0.4738 - val_TP: 13.8182 - val_FN: 2.8182 - val_TN: 9.8409 - val_FP: 5.2273\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 101us/step - loss: 0.1788 - acc: 0.7333 - precision: 0.7055 - recall: 0.7969 - f1: 0.7410 - TP: 12.6718 - FN: 3.2137 - TN: 10.7532 - FP: 5.3079 - val_loss: 0.1769 - val_acc: 0.7369 - val_precision: 0.5227 - val_recall: 0.3967 - val_f1: 0.4503 - val_TP: 12.6364 - val_FN: 4.0000 - val_TN: 10.7273 - val_FP: 4.3409\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1760 - acc: 0.7395 - precision: 0.7163 - recall: 0.8022 - f1: 0.7487 - TP: 12.7099 - FN: 3.1756 - TN: 10.9135 - FP: 5.1476 - val_loss: 0.1723 - val_acc: 0.7477 - val_precision: 0.5227 - val_recall: 0.4434 - val_f1: 0.4794 - val_TP: 14.1136 - val_FN: 2.5227 - val_TN: 9.5909 - val_FP: 5.4773\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 109us/step - loss: 0.1749 - acc: 0.7454 - precision: 0.7195 - recall: 0.8044 - f1: 0.7527 - TP: 12.7939 - FN: 3.0916 - TN: 11.0178 - FP: 5.0433 - val_loss: 0.1785 - val_acc: 0.7341 - val_precision: 0.5218 - val_recall: 0.4740 - val_f1: 0.4963 - val_TP: 15.0909 - val_FN: 1.5455 - val_TN: 8.1818 - val_FP: 6.8864\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 101us/step - loss: 0.1752 - acc: 0.7427 - precision: 0.7159 - recall: 0.8073 - f1: 0.7508 - TP: 12.8142 - FN: 3.0712 - TN: 10.9109 - FP: 5.1501 - val_loss: 0.1757 - val_acc: 0.7412 - val_precision: 0.5227 - val_recall: 0.3925 - val_f1: 0.4476 - val_TP: 12.5000 - val_FN: 4.1364 - val_TN: 11.0000 - val_FP: 4.0682\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 100us/step - loss: 0.1744 - acc: 0.7459 - precision: 0.7203 - recall: 0.8125 - f1: 0.7556 - TP: 12.8728 - FN: 3.0127 - TN: 10.9567 - FP: 5.1043 - val_loss: 0.1803 - val_acc: 0.7305 - val_precision: 0.5219 - val_recall: 0.4834 - val_f1: 0.5016 - val_TP: 15.3864 - val_FN: 1.2500 - val_TN: 7.7727 - val_FP: 7.2955\n",
      "train_auc:  0.8360980775020621\n",
      "test_auc:  0.8192610587740772\n",
      "\n",
      "\n",
      "i:  7\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 2s 125us/step - loss: 0.2054 - acc: 0.6809 - precision: 0.6516 - recall: 0.7693 - f1: 0.6910 - TP: 12.4071 - FN: 3.6336 - TN: 9.3461 - FP: 6.5598 - val_loss: 0.2105 - val_acc: 0.6667 - val_precision: 0.4773 - val_recall: 0.4474 - val_f1: 0.4616 - val_TP: 14.2955 - val_FN: 0.9545 - val_TN: 6.8409 - val_FP: 9.6136\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 100us/step - loss: 0.1905 - acc: 0.7110 - precision: 0.6832 - recall: 0.7972 - f1: 0.7269 - TP: 12.7990 - FN: 3.2417 - TN: 9.9160 - FP: 5.9898 - val_loss: 0.1868 - val_acc: 0.7254 - val_precision: 0.4773 - val_recall: 0.4203 - val_f1: 0.4464 - val_TP: 13.4318 - val_FN: 1.8182 - val_TN: 9.5682 - val_FP: 6.8864\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 101us/step - loss: 0.1856 - acc: 0.7233 - precision: 0.7000 - recall: 0.7941 - f1: 0.7354 - TP: 12.7405 - FN: 3.3003 - TN: 10.3664 - FP: 5.5394 - val_loss: 0.2000 - val_acc: 0.6953 - val_precision: 0.4773 - val_recall: 0.4438 - val_f1: 0.4596 - val_TP: 14.1818 - val_FN: 1.0682 - val_TN: 7.8636 - val_FP: 8.5909\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 100us/step - loss: 0.1831 - acc: 0.7293 - precision: 0.7066 - recall: 0.7985 - f1: 0.7410 - TP: 12.7812 - FN: 3.2595 - TN: 10.5165 - FP: 5.3893 - val_loss: 0.1751 - val_acc: 0.7484 - val_precision: 0.4773 - val_recall: 0.3727 - val_f1: 0.4175 - val_TP: 11.9091 - val_FN: 3.3409 - val_TN: 11.8182 - val_FP: 4.6364\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 100us/step - loss: 0.1813 - acc: 0.7298 - precision: 0.7055 - recall: 0.7972 - f1: 0.7400 - TP: 12.7964 - FN: 3.2443 - TN: 10.5191 - FP: 5.3868 - val_loss: 0.1746 - val_acc: 0.7491 - val_precision: 0.4773 - val_recall: 0.3798 - val_f1: 0.4219 - val_TP: 12.1364 - val_FN: 3.1136 - val_TN: 11.6136 - val_FP: 4.8409\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 100us/step - loss: 0.1793 - acc: 0.7354 - precision: 0.7101 - recall: 0.8028 - f1: 0.7461 - TP: 12.8931 - FN: 3.1476 - TN: 10.6005 - FP: 5.3053 - val_loss: 0.1797 - val_acc: 0.7434 - val_precision: 0.4773 - val_recall: 0.4196 - val_f1: 0.4461 - val_TP: 13.4091 - val_FN: 1.8409 - val_TN: 10.1591 - val_FP: 6.2955\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1779 - acc: 0.7384 - precision: 0.7160 - recall: 0.8029 - f1: 0.7482 - TP: 12.8702 - FN: 3.1705 - TN: 10.7201 - FP: 5.1858 - val_loss: 0.1753 - val_acc: 0.7527 - val_precision: 0.4773 - val_recall: 0.4139 - val_f1: 0.4425 - val_TP: 13.2273 - val_FN: 2.0227 - val_TN: 10.6364 - val_FP: 5.8182\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 104us/step - loss: 0.1783 - acc: 0.7368 - precision: 0.7122 - recall: 0.8043 - f1: 0.7481 - TP: 12.9135 - FN: 3.1272 - TN: 10.6260 - FP: 5.2799 - val_loss: 0.1835 - val_acc: 0.7297 - val_precision: 0.4773 - val_recall: 0.4253 - val_f1: 0.4493 - val_TP: 13.5909 - val_FN: 1.6591 - val_TN: 9.5455 - val_FP: 6.9091\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 98us/step - loss: 0.1768 - acc: 0.7397 - precision: 0.7161 - recall: 0.8019 - f1: 0.7484 - TP: 12.8499 - FN: 3.1908 - TN: 10.7812 - FP: 5.1247 - val_loss: 0.1854 - val_acc: 0.7312 - val_precision: 0.4773 - val_recall: 0.4360 - val_f1: 0.4553 - val_TP: 13.9318 - val_FN: 1.3182 - val_TN: 9.2500 - val_FP: 7.2045\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 99us/step - loss: 0.1776 - acc: 0.7401 - precision: 0.7142 - recall: 0.8061 - f1: 0.7502 - TP: 12.9237 - FN: 3.1170 - TN: 10.7201 - FP: 5.1858 - val_loss: 0.1719 - val_acc: 0.7548 - val_precision: 0.4773 - val_recall: 0.3926 - val_f1: 0.4300 - val_TP: 12.5455 - val_FN: 2.7045 - val_TN: 11.3864 - val_FP: 5.0682\n",
      "train_auc:  0.8284404952060462\n",
      "test_auc:  0.8239917744604821\n",
      "\n",
      "\n",
      "i:  8\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 2s 127us/step - loss: 0.2115 - acc: 0.6719 - precision: 0.6519 - recall: 0.7744 - f1: 0.6926 - TP: 12.4377 - FN: 3.6209 - TN: 9.0280 - FP: 6.8601 - val_loss: 0.2006 - val_acc: 0.6810 - val_precision: 0.4744 - val_recall: 0.4205 - val_f1: 0.4453 - val_TP: 13.2955 - val_FN: 1.7955 - val_TN: 8.2955 - val_FP: 8.3182\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 98us/step - loss: 0.1923 - acc: 0.7074 - precision: 0.6809 - recall: 0.7972 - f1: 0.7241 - TP: 12.8092 - FN: 3.2494 - TN: 9.7888 - FP: 6.0992 - val_loss: 0.1855 - val_acc: 0.7183 - val_precision: 0.4761 - val_recall: 0.3802 - val_f1: 0.4220 - val_TP: 12.0227 - val_FN: 3.0682 - val_TN: 10.7500 - val_FP: 5.8636\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 102us/step - loss: 0.1853 - acc: 0.7215 - precision: 0.6952 - recall: 0.8031 - f1: 0.7350 - TP: 12.8931 - FN: 3.1654 - TN: 10.1578 - FP: 5.7303 - val_loss: 0.1823 - val_acc: 0.7240 - val_precision: 0.4773 - val_recall: 0.3383 - val_f1: 0.3946 - val_TP: 10.7045 - val_FN: 4.3864 - val_TN: 12.2500 - val_FP: 4.3636\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 96us/step - loss: 0.1813 - acc: 0.7334 - precision: 0.7070 - recall: 0.8096 - f1: 0.7468 - TP: 13.0051 - FN: 3.0534 - TN: 10.4249 - FP: 5.4631 - val_loss: 0.2051 - val_acc: 0.6867 - val_precision: 0.4773 - val_recall: 0.2029 - val_f1: 0.2830 - val_TP: 6.4091 - val_FN: 8.6818 - val_TN: 15.3636 - val_FP: 1.2500\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 98us/step - loss: 0.1800 - acc: 0.7325 - precision: 0.7080 - recall: 0.8009 - f1: 0.7440 - TP: 12.8601 - FN: 3.1985 - TN: 10.5420 - FP: 5.3461 - val_loss: 0.1755 - val_acc: 0.7405 - val_precision: 0.4752 - val_recall: 0.3968 - val_f1: 0.4319 - val_TP: 12.5455 - val_FN: 2.5455 - val_TN: 10.9318 - val_FP: 5.6818\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 95us/step - loss: 0.1770 - acc: 0.7421 - precision: 0.7185 - recall: 0.8096 - f1: 0.7530 - TP: 12.9822 - FN: 3.0763 - TN: 10.7252 - FP: 5.1628 - val_loss: 0.1735 - val_acc: 0.7341 - val_precision: 0.4759 - val_recall: 0.3568 - val_f1: 0.4072 - val_TP: 11.2955 - val_FN: 3.7955 - val_TN: 11.9773 - val_FP: 4.6364\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 96us/step - loss: 0.1762 - acc: 0.7406 - precision: 0.7172 - recall: 0.8076 - f1: 0.7515 - TP: 12.9364 - FN: 3.1221 - TN: 10.7226 - FP: 5.1654 - val_loss: 0.1973 - val_acc: 0.6953 - val_precision: 0.4744 - val_recall: 0.4418 - val_f1: 0.4571 - val_TP: 13.9773 - val_FN: 1.1136 - val_TN: 8.0682 - val_FP: 8.5455\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 95us/step - loss: 0.1752 - acc: 0.7456 - precision: 0.7221 - recall: 0.8111 - f1: 0.7560 - TP: 13.0051 - FN: 3.0534 - TN: 10.8142 - FP: 5.0738 - val_loss: 0.1787 - val_acc: 0.7341 - val_precision: 0.4752 - val_recall: 0.4181 - val_f1: 0.4445 - val_TP: 13.2273 - val_FN: 1.8636 - val_TN: 10.0455 - val_FP: 6.5682\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 96us/step - loss: 0.1760 - acc: 0.7435 - precision: 0.7183 - recall: 0.8121 - f1: 0.7543 - TP: 13.0127 - FN: 3.0458 - TN: 10.7405 - FP: 5.1476 - val_loss: 0.1728 - val_acc: 0.7391 - val_precision: 0.4759 - val_recall: 0.3390 - val_f1: 0.3945 - val_TP: 10.7273 - val_FN: 4.3636 - val_TN: 12.7045 - val_FP: 3.9091\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 96us/step - loss: 0.1729 - acc: 0.7487 - precision: 0.7267 - recall: 0.8102 - f1: 0.7588 - TP: 13.0000 - FN: 3.0585 - TN: 10.9186 - FP: 4.9695 - val_loss: 0.1692 - val_acc: 0.7491 - val_precision: 0.4747 - val_recall: 0.3660 - val_f1: 0.4128 - val_TP: 11.5909 - val_FN: 3.5000 - val_TN: 12.1591 - val_FP: 4.4545\n",
      "train_auc:  0.8378472615916953\n",
      "test_auc:  0.8303003395249946\n",
      "\n",
      "\n",
      "i:  9\n",
      "model\n",
      "compile\n",
      "Train on 12555 samples, validate on 1395 samples\n",
      "Epoch 1/10\n",
      "12555/12555 [==============================] - 1s 119us/step - loss: 0.2213 - acc: 0.6490 - precision: 0.6277 - recall: 0.7089 - f1: 0.6384 - TP: 11.3537 - FN: 4.6107 - TN: 9.3791 - FP: 6.6031 - val_loss: 0.2007 - val_acc: 0.6882 - val_precision: 0.4984 - val_recall: 0.4572 - val_f1: 0.4765 - val_TP: 14.5682 - val_FN: 1.3636 - val_TN: 7.2500 - val_FP: 8.5227\n",
      "Epoch 2/10\n",
      "12555/12555 [==============================] - 1s 96us/step - loss: 0.1968 - acc: 0.6981 - precision: 0.6774 - recall: 0.7655 - f1: 0.7076 - TP: 12.2112 - FN: 3.7532 - TN: 10.0916 - FP: 5.8906 - val_loss: 0.1858 - val_acc: 0.7290 - val_precision: 0.4991 - val_recall: 0.3973 - val_f1: 0.4417 - val_TP: 12.6591 - val_FN: 3.2727 - val_TN: 10.4545 - val_FP: 5.3182\n",
      "Epoch 3/10\n",
      "12555/12555 [==============================] - 1s 97us/step - loss: 0.1913 - acc: 0.7075 - precision: 0.6865 - recall: 0.7732 - f1: 0.7166 - TP: 12.3486 - FN: 3.6158 - TN: 10.2545 - FP: 5.7277 - val_loss: 0.1870 - val_acc: 0.7211 - val_precision: 0.4984 - val_recall: 0.4472 - val_f1: 0.4710 - val_TP: 14.2500 - val_FN: 1.6818 - val_TN: 8.6136 - val_FP: 7.1591\n",
      "Epoch 4/10\n",
      "12555/12555 [==============================] - 1s 94us/step - loss: 0.1860 - acc: 0.7219 - precision: 0.6963 - recall: 0.7914 - f1: 0.7324 - TP: 12.6539 - FN: 3.3104 - TN: 10.4097 - FP: 5.5725 - val_loss: 0.1845 - val_acc: 0.7262 - val_precision: 0.4984 - val_recall: 0.4472 - val_f1: 0.4709 - val_TP: 14.2500 - val_FN: 1.6818 - val_TN: 8.7727 - val_FP: 7.0000\n",
      "Epoch 5/10\n",
      "12555/12555 [==============================] - 1s 94us/step - loss: 0.1837 - acc: 0.7277 - precision: 0.7030 - recall: 0.7935 - f1: 0.7369 - TP: 12.6667 - FN: 3.2977 - TN: 10.5802 - FP: 5.4020 - val_loss: 0.1785 - val_acc: 0.7398 - val_precision: 0.5000 - val_recall: 0.3858 - val_f1: 0.4345 - val_TP: 12.2955 - val_FN: 3.6364 - val_TN: 11.1591 - val_FP: 4.6136\n",
      "Epoch 6/10\n",
      "12555/12555 [==============================] - 1s 94us/step - loss: 0.1797 - acc: 0.7337 - precision: 0.7104 - recall: 0.7930 - f1: 0.7414 - TP: 12.6641 - FN: 3.3003 - TN: 10.7761 - FP: 5.2061 - val_loss: 0.1827 - val_acc: 0.7348 - val_precision: 0.5000 - val_recall: 0.3444 - val_f1: 0.4070 - val_TP: 10.9773 - val_FN: 4.9545 - val_TN: 12.3182 - val_FP: 3.4545\n",
      "Epoch 7/10\n",
      "12555/12555 [==============================] - 1s 94us/step - loss: 0.1775 - acc: 0.7391 - precision: 0.7139 - recall: 0.7989 - f1: 0.7461 - TP: 12.7684 - FN: 3.1959 - TN: 10.8422 - FP: 5.1399 - val_loss: 0.1732 - val_acc: 0.7462 - val_precision: 0.4991 - val_recall: 0.4165 - val_f1: 0.4534 - val_TP: 13.2727 - val_FN: 2.6591 - val_TN: 10.3864 - val_FP: 5.3864\n",
      "Epoch 8/10\n",
      "12555/12555 [==============================] - 1s 99us/step - loss: 0.1788 - acc: 0.7362 - precision: 0.7109 - recall: 0.8032 - f1: 0.7469 - TP: 12.8473 - FN: 3.1170 - TN: 10.6718 - FP: 5.3104 - val_loss: 0.1738 - val_acc: 0.7412 - val_precision: 0.4991 - val_recall: 0.4286 - val_f1: 0.4606 - val_TP: 13.6591 - val_FN: 2.2727 - val_TN: 9.8409 - val_FP: 5.9318\n",
      "Epoch 9/10\n",
      "12555/12555 [==============================] - 1s 95us/step - loss: 0.1764 - acc: 0.7407 - precision: 0.7146 - recall: 0.8109 - f1: 0.7519 - TP: 12.9135 - FN: 3.0509 - TN: 10.7481 - FP: 5.2341 - val_loss: 0.1723 - val_acc: 0.7448 - val_precision: 0.4991 - val_recall: 0.4158 - val_f1: 0.4528 - val_TP: 13.2500 - val_FN: 2.6818 - val_TN: 10.3636 - val_FP: 5.4091\n",
      "Epoch 10/10\n",
      "12555/12555 [==============================] - 1s 96us/step - loss: 0.1748 - acc: 0.7449 - precision: 0.7228 - recall: 0.8009 - f1: 0.7520 - TP: 12.7812 - FN: 3.1832 - TN: 11.0153 - FP: 4.9669 - val_loss: 0.1823 - val_acc: 0.7391 - val_precision: 0.5000 - val_recall: 0.3321 - val_f1: 0.3977 - val_TP: 10.5909 - val_FN: 5.3409 - val_TN: 12.8409 - val_FP: 2.9318\n",
      "train_auc:  0.834628150018243\n",
      "test_auc:  0.8253688637475488\n"
     ]
    }
   ],
   "source": [
    "# split data and output result\n",
    "data = np.array(pd.read_csv(\"6fea_idx12Scale_infoGainSorted50.csv\"))#inputfile\n",
    "X1 = data[0:6975, 1:]#6975 is the number of positive samples in training set, '1' is the label of positive sample\n",
    "Y1 = data[0:6975, 0]#'0' is the label of negative sample\n",
    "X2 = data[6975:, 1:]\n",
    "Y2 = data[6975:, 0]\n",
    "X = np.concatenate([X1, X2], 0)\n",
    "Y = np.concatenate([Y1, Y2], 0)\n",
    "#Y = Y.reshape((Y.shape[0], -1))\n",
    "print (X)\n",
    "print (\"X.shape: \", X.shape)\n",
    "print (\"Y.shape: \", Y.shape)\n",
    "\n",
    "lr = 0.2 #learning rate\n",
    "epoch = 10\n",
    "batch_size = 32\n",
    "kf = KFold(n_splits = 10, shuffle = True, random_state = 42)\n",
    "#kf = KFold(n_splits = 5, shuffle = False)\n",
    "kf = kf.split(X)\n",
    "\n",
    "test_aucs = []\n",
    "for i, (train_fold, validate_fold) in enumerate(kf):\n",
    "    print(\"\\n\\ni: \", i)\n",
    "    test_auc = dnn_model(X[train_fold], Y[train_fold], X[validate_fold], Y[validate_fold], lr, epoch, batch_size)\n",
    "    test_aucs.append(test_auc)\n",
    "w = open(\"6fea_idx12Scale_infoGainSorted50Result.txt\", \"w\")\n",
    "for j in test_aucs: \n",
    "    w.write(str(j) + ',')\n",
    "w.write('\\n')\n",
    "w.write(str(np.mean(test_aucs)) + '\\n')\n",
    "w.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
